# 第二章：感知机

​		感知机 (perceptron) 类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1 值。

​	判别模型，线性划分分离超平面。

## 2.1 感知机模型

​		定义 2.1 感知机 假设输入空间(特征空间)是$X \subseteq \mathbb{R^n}$，输出空间是$Y =\{+1, -1\}$ 。输 入$x \in X$表示实例的特征向量，对应于输入空间(特征空间)的点；输出$y \in Y$表示实例的类别 由输入空间到输出空间的如下函数:
$$
f(x) = sign(w \cdot x + b)
$$
​		称为感知机。

![图2.1 感知机模型](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE2.1.PNG)



## 2.2 感知机学习策略

### 2.1.1 数据集的线性可分性

### 2.2.2 感知机学习策略

​		**损失函数**：误分类点到超平面的总距离。

​		$x_0$到超平面S的距离：
$$
\frac {1}{||w||} |w \cdot x_0 + b|
$$
​		$||w||$是$w$的$L_2$范数。

​		所有误分类点到超平面$S$的总距离为：
$$
- \frac {1}{||w||} \sum_{x_i \in M}y_i(w \cdot x_i + b)
$$
​		不考虑$\frac {1}{||w||}$，就得到感知机学习的损失函数。

​		给定训练数据集
$$
T = \{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}
$$
​		其中，$x_i \in X = \mathbb{R^n}$，$y_i \in Y = \{+1, -1\}$，$i=1,2,...,N$。感知机$sign(w \cdot x + b)$学习的损失函数定义为
$$
L(w,b) = - \sum_{x_i \in M}y_i(w \cdot x_i + b)
$$
​		其中$M$为误分类点的集合，这个**损失函数**是感知机学习的经验风险函数。



## 2.3 感知机学习算法

​		求解损失函数的最优化问题——随机梯度下降

### 2.3.1 感知机学习算法的原始形式

$$
\underset{w,b}{L(w,b)}= - \sum_{x_i \in M}y_i(w \cdot x_i + b)
$$

​		感知机学习算法是由误分类驱动的，具体采用随机梯度下降法(stochastic gradient decent)。

​		假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由
$$
\bigtriangledown _w L(w,b)=-\sum _{x_i \in M}y_ix_i
$$

$$
\bigtriangledown _b L(w,b)=-\sum _{x_i \in M}y_i
$$

​		随机选择一个误分类点$(x_i, y_i)$对$w,b$进行更新:
$$
w \leftarrow w +\eta y_i x_i
$$

$$
b \leftarrow b +\eta y_i
$$

**算法2.1（感知机算法的原始形式）**

​		输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in X = \mathbb{R^n}$，$y_i \in Y = \{-1, +1\}$，$i=1,2,...,N$，学习率$\eta(0<\eta<=1)$；

​		输出：$w,b$；感知机模型$f(x)=sign(w \cdot x + b)$。

​		(1)选取初值$w_0, b_0$；

​		(2)在训练集中选取数据$(x_i, y_i)$；

​		(3)如果$y_i(w \cdot x + b \leqslant 0) $，
$$
w \leftarrow w +\eta y_i x_i
$$

$$
b \leftarrow b +\eta y_i
$$

​		(4)转至(2)，直至训练集中没有误分类点。

### 2.3.2 算法的收敛性

​		对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次选代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。

### 2.3.3 感知机学习算法的对偶形式

感知机学习算法的原始形式和对偶形式与第七章中支持向量机学习算法的原始形式和对偶形式相对应。

![算法2.2(感知机学习算法的对偶形式)](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%952.2.PNG)



Gram矩阵
$$
G = \begin{bmatrix}
x_i \cdot x_j
\end{bmatrix}_{N\times N}
$$
