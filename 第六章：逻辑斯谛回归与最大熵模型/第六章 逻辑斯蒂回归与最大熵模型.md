[TOC]



# 第六章 逻辑斯谛回归与最大熵模型

**逻辑斯谛回归**(logistic regression)是统计学习中的经典分类方法。**最大熵**是概率模型学习的一个准则，将其推广到分类问题得到**最大熵模型**(maximum entropy model)。逻辑斯谛回归模型与最大熵模型都属于**对数线性模型**。

## 6.1 逻辑斯谛回归模型

### 6.1.1 逻辑斯谛分布

![定义6.1（逻辑斯谛分布）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%896.1%EF%BC%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%88%86%E5%B8%83%EF%BC%89.PNG)

![图6.1逻辑斯谛分布的密度函数与分布函数](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.1%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E4%B8%8E%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0.PNG)

#### 6.1.2 二项逻辑斯谛回归模型

![定义6.2（逻辑斯谛回归模型）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%896.2%EF%BC%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%89.PNG)

为了方便将权值向量和输入向量加以扩充，仍记作$w$，$x$，即$w=(w^{(1)},w^{(2)},...,w^{(n)},b)$，$x=(x^{(1)},x^{(2)},...,x^{(n)},1)$。这时逻辑斯谛回归模型如下：
$$
P(Y=1|x)=\frac{exp(w \cdot x)}{1+exp(w \cdot x)} \\
P(Y=0|x)=\frac{1}{1+exp(w \cdot x)}
$$


一个事件的儿率(odds)是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的**对数几率**(log odds)或logit函数是
$$
logit(p)=log{\frac{p}{1-p}}
$$
对逻辑斯谛回归而言：
$$
log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w \cdot x
$$
**输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型**，即逻辑斯谛回归模型。

线性函数的值越接近正无穷，概率值就越接近 1，线性函数的值越接近负无穷，概率值就越接近0(如图 6.1 示)。这样的模型就是逻辑斯谛回归模型。

### 6.1.3 模型参数估计

**极大似然估计**

![6.1.3模型参数估计](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.1.3%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.PNG)

通常采用**梯度下降法**或**拟牛顿法**

### 6.1.4 多项逻辑斯谛回归

![6.1.4多项逻辑斯谛回归模型](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.1.4%E5%A4%9A%E9%A1%B9%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B.PNG)



## 6.2 最大熵模型

最大熵模型(maximum entropy model)由最大熵原理推导实现。

### 6.2.1 最大熵原理

最大熵原理是**概率模型学习**的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为**在满足约束条件的模型集合中选取值最大的模型**。

假设离散随机变量$X$的概率分布是$P(X)$，则其熵（参照5.2.2节）是
$$
H(P)=-\underset {x}{\sum}P(x)logP(x)
$$
熵满足下列不等式：
$$
0<=H(P)<=log|X|
$$
$|X|$是$X$的取值个数，当$X$服从均匀分布时，熵最大。

直观地，最大熵原理认为要选择的概率模型首先必须满足己有的事实，即**约束条件。在没有更多信息的情况下，那些不确定的部分都是"等可能的"**。 

图6.2提供了用最大熵原理进行概率模型选择的几何解释。

![图6.2概率模型集合](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.2%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B%E9%9B%86%E5%90%88.PNG)

### 6.2.3 最大熵模型的定义

最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。

联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$的经验分布。

![图6.2.2最大熵模型的定义](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.2.2%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9A%E4%B9%89.PNG)

如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即
$$
E_P(f)=E_{\tilde{P}}(f)
$$
或
$$
\underset{x,y}{\sum} \tilde{P}(x)P(y|x)f(x,y)=\underset{x,y}{\sum}\tilde{P}(x,y)f(x.y)
$$
我们将上式或者上上式作为**模型的约束条件**。假如有n个特征函数$f_i(x,y),i=1,2,...,n$，那么就有n个约束条件。

![定义6.3（最大熵模型）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%896.3%EF%BC%88%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%EF%BC%89.PNG)

则模型集合$\C$中**条件熵$H(P)$最大的模型称为最大熵模型**，式中的对数为**自然对数**。

### 6.2.3 最大熵模型的学习

最大熵模型的学习可以形式化为**约束最优化问题**。

对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$以及特征函数$f_i(x,y),i=1,2,...,n$，最大熵模型的学习等价于约束最优化问题：
$$
\underset {P\in \C}{max}H(P)=-\underset{x,y}{\sum} \tilde{P}(x)P(y|x)logP(y|x) \\
s.t. E_P(f_i)=E_{\tilde{P}}(f_i), i=1,2,...,n \\
\underset{y}{\sum}P(y|x)=1
$$
最优化习惯，将求最大值问题改写为等价的求最小值问题：
$$
\underset {P\in \C}{min}H(P)=\underset{x,y}{\sum} \tilde{P}(x)P(y|x)logP(y|x) \\
s.t. E_P(f_i)-E_{\tilde{P}}(f_i)=0, i=1,2,...,n \\
\underset{y}{\sum}P(y|x)=1
$$
将**约束最优化的原始问题**转换为**无约束最优化的对偶问题**。（拉格朗日乘数法+KKT条件）

引入拉格朗日乘子$w_0,w_1,...,w_n$，定义拉格朗日函数$L(P,w)$：
$$
L(P,w)=-H(P)+w_0(1-\underset{y}{\sum}P(y|x)) +\sum_{i=1}^{n}w_i(E_{\tilde{P}}(f_i)-E_P(f_i)) \\
=\underset{x,y}{\sum} \tilde{P}(x)P(y|x)logP(y|x) + w_0(1-\underset{y}{\sum}P(y|x)) +\\ \sum_{i=1}^{n}w_i(\underset{x,y}{\sum}\tilde{P}(x,y)f(x.y)-\underset{x,y}{\sum} \tilde{P}(x)P(y|x)f(x,y))
$$
**最优化的原始问题**是
$$
\underset{P \in \C}{min} \underset{w}{max}L(P,w)\\
$$
**对偶问题**是
$$
\underset{w}{max} \underset{P \in \C}{min}L(P,w)
$$
由于**拉格朗日函数$L(P,w)$是$P$的凸函数**，原始问题的解与对偶问题的解是等价的。

求内部的极小化问题，记对偶函数$\Psi(w)$
$$
\Psi(w)=\underset{P \in \C}{min}L(P,w)=L(P_w,w)
$$
同时将其解记作
$$
P_w=arg \underset{w}{min}L(P,w)=P_w(y|x)
$$

具体地，求$L(P,w)$对$P(y|x)$的偏导数，并令偏导数为0，在$\tilde{P}(x)>0$的情况下解得
$$
P(y|x)=exp(\sum_{i=1}^{n}w_i f_i(x,y) + w_0 -1)=\frac{exp(\sum_{i=1}^{n}w_i f_i(x,y))}{exp(1-w_0)}
$$
由于$\sum_{y}P{y|x}=1$，得
$$
P_w(y|x)=\frac{1}{Z_w(x)}exp(\sum_{i=1}^{n}w_i f_i(x,y))
$$
其中
$$
Z_w(x)=\sum_y exp(\sum_{i=1}^{n}w_i f_i(x,y))
$$
$Z_w(x)$称为规范化因子；$f_i(x,y)$是特征函数；$w_i$是特征的权值。

之后再求解对偶问题外部的极大化问题
$$
\underset{w}{max}\Psi(w)
$$
将其解记为$w^*$，即
$$
w^*=\underset{w}{argmax}\Psi(w)
$$
**整个过程：**
$$
L(P,w)对P(y|x)求导-->P_w(y|x)\underset{代回}{--}>L(P,w)对w求导-->P_w(y|x)
$$


### 6.2.4 极大似然估计

下面证明对偶函数的极大化等价于最大熵模型的极大似然估计。

![图6.2.4极大似然估计](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.2.4%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.PNG)

最大熵模型更一般的形式
$$
P_w(y|x)=\frac{1}{Z_w(x)}exp(\sum_{i=1}^{n}w_i f_i(x,y))
$$

$$
Z_w(x)=\sum_y exp(\sum_{i=1}^{n}w_i f_i(x,y))
$$



## 6.3 模型学习的最优化算法

逻辑斯谛回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。牛顿法或拟牛顿法一般收敛速度更快。

#### 6.3.1 改进的迭代尺度法

改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。

已知最大熵模型
$$
P_w(y|x)=\frac{1}{Z_w(x)}exp(\sum_{i=1}^{n}w_i f_i(x,y))
$$

$$
Z_w(x)=\sum_y exp(\sum_{i=1}^{n}w_i f_i(x,y))
$$

对数似然函数
$$
L(w)=\underset{x,y}{\sum} \tilde{P}(x,y) \sum_{i=1}^{n}w_i f_i(x,y) - \underset{x}{\sum}\tilde{P}(x)logZ_w(x)
$$
$w\rightarrow w+\delta$**，使得似然函数增加**
$$
L(w+\delta)-L(w)=\underset{x,y}{\sum} \tilde{P}(x,y) \sum_{i=1}^{n}\delta_i f_i(x,y) - \underset{x}{\sum}\tilde{P}(x)\frac{logZ_{w+\delta}(x)}{logZ_w(x)}
$$
利用不等式
$$
-log\alpha >= 1 - \alpha, \alpha>0
$$
对数似然函数的下界：
$$
L(w+\delta)-L(w)\\
>=\underset{x,y}{\sum} \tilde{P}(x,y) \sum_{i=1}^{n}\delta_i f_i(x,y) - \underset{x}{\sum}\tilde{P}(x) \underset{y}{\sum}P_w(y|x)exp \sum_{i=1}^{n}\delta_i f_i(x,y)\\
=A(\delta|w)
$$
使得下界提高。IIS试图一次只优化其中一个变量$\delta_i$，而固定其他变量。

为了达到这一目的，**IIS**进一步降低下界$A(\delta|w)$。具体地，**IIS**引入一个量$f^\#(x,y)$
$$
f^\#(x,y)=\underset{i}{\sum}f_i(x,y)
$$
故$A(\delta|w)$可改写为
$$
A(\delta|w)=\underset{x,y}{\sum} \tilde{P}(x,y) \sum_{i=1}^{n}\delta_i f_i(x,y) - \underset{x}{\sum}\tilde{P}(x) \underset{y}{\sum}P_w(y|x) exp (f^\#(x,y) \sum_{i=1}^{n}\frac{\delta_i f_i(x,y)}{f^\#(x,y)})
$$
根据**Jensen不等式**
$$
A(\delta|w) \\
>=\underset{x,y}{\sum} \tilde{P}(x,y) \sum_{i=1}^{n}\delta_i f_i(x,y) - \underset{x}{\sum}\tilde{P}(x) \underset{y}{\sum}P_w(y|x) \sum_{i=1}^{n}\frac{f_i(x,y)}{f^\#(x,y)} exp (\delta_i f^\#(x,y)) \\
=B(\delta|w)
$$
$B(\delta|w)$**是对数似然函数该变量的一个新的（相对不紧的）下界。**

$B(\delta|w)$对$\delta_i$求偏导，偏导数中不含$\delta_i$以外的其他变量，令其为0得到
$$
 \underset{x,y}{\sum} \tilde{P}(x) P_w(y|x) f_i(x,y) exp (\delta_i f^\#(x,y)) = E_{\tilde{P}}(f_i)
$$
![算法6.1(改进的迭代尺度算法IIS)](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%956.1(%E6%94%B9%E8%BF%9B%E7%9A%84%E8%BF%AD%E4%BB%A3%E5%B0%BA%E5%BA%A6%E7%AE%97%E6%B3%95IIS).PNG)

#### 6.3.2 拟牛顿法

![图6.3.2最大熵模型的拟牛顿法](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE6.3.2%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95.PNG)