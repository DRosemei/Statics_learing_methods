[TOC]

# 第七章 支持向量机

支持向量机(support vector machines, SVM) 是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，**间隔最大**使它有别于感知机；支持向量机还包括**核技巧**，这使它成为实质上的非线性分类器。

线性可分支持向量机(linear support vector machine in linearly separable case) 、线性支持向量机(linear support vector machine) 以及非线性支持向量(non-linear support vector machine) 。

当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，核函数(kernel function)表示**将输入从输入空间映射到特征空间得到的特征向量之间的内积**。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。



## 7.1 线性可分支持向量机与硬间隔最大化

### 7.1.1 线性可分支持向量机

![图7.1.1一些前提定义](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE7.1.1%E4%B8%80%E4%BA%9B%E5%89%8D%E6%8F%90%E5%AE%9A%E4%B9%89.PNG)

一般地，训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。**感知机**利用**误分类最小**的策略，求得分离超平面。不过这时的解有**无穷**多个。**线性可分支持向量机**利用**间隔最大化**求最优分离超平面，这时，**解是唯一**的。

![定义7.1（线性可分支持向量机）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%89%207.1%EF%BC%88%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%89.PNG)

### 7.1.2 函数间隔和几何间隔

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。

![定义7.2（函数间隔）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%897.2%EF%BC%88%E5%87%BD%E6%95%B0%E9%97%B4%E9%9A%94%EF%BC%89.PNG)

函数间隔可以表示分类预测的正确性及确信度。但是选择分离超平面时，只有函数间隔还不够。因为只要成比例地改变$w,b$，例如将它们改为$2w,2b$，平面并没有改变，但函数间隔却成为原来的2倍。故使用几何间隔，对$w$加以约束。

![定义7.3（几何间隔）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%897.3%EF%BC%88%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94%EF%BC%89.PNG)

超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离(signed distance)，当样本点被超平面正确分类时就是实例点到超平面的距离。

函数间隔$\hat{\gamma}$和几何间隔$\gamma$
$$
\gamma_i=\frac{\hat{\gamma_i}}{||w||} \\
\gamma=\frac{\hat{\gamma}}{||w||} \\
$$


### 7.1.3 间隔最大化

间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且**对最难分的实例点(离超平面最近的点)也有足够大的确信度将它们分开**。这样的超平面应该对未知的新实例有很好的分类预测能力。

1. **最大间隔分离超平面**

   约束最优化问题
   $$
   \underset{w,b}{max} {\space} \gamma \\
   s.t.{\space} y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})>=\gamma, \space i=1,2,...,N
   $$
   函数间隔
   $$
   \underset{w,b}{max} {\space} \hat{\gamma} \\
   s.t.{\space} y_i(w \cdot x_i + w)>=\hat{\gamma}, \space i=1,2,...,N
   $$
   函数间隔$\hat{\gamma}$的取值并不影响最优化问题的解。取$\hat{\gamma}=1$，并最小化$\frac{1}{2}||w||^2$

   得到**线性可分支持向量机学习的最优化问题**
   $$
   \underset{w,b}{min} {\space}  \frac{1}{2}||w||^2 \\
   s.t.{\space} y_i(w \cdot x_i + w) - 1 >= 0, \space i=1,2,...,N
   $$
   这是一个**凸二次规划**问题。

   凸优化问题是指约束最优化问题
   $$
   \underset{w}{min} \space f(w) \\
   s.t. \space g_i(w) <= 0, \space i=1,2,...,k \\
   h_j(w) =0, \space j=1,2,...,l
   $$
   其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微的凸函数，约束函数$h_j(w)$是$R^n$上的仿射函数。（$f(x)$是仿射函数，如果它满足$f(x)=a \cdot x + b, a \in R^n, b \in R^n, x \in R^n$）

   当目标函数$f(w)$是二次函数且约束函数$g_i(w)$是仿射函数时，称为凸二次规划问题。

   ![算法7.1（线性可分支持向量机学习算法——最大间隔法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%957.1%EF%BC%88%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94%E6%B3%95%EF%BC%89.PNG)

   

2. **最大间隔分离超平面的存在唯一性**

   **定理7.1（最大间隔分离超平面的存在唯一性）** 若训练数据集$T$线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

   证明见书p117

   

3. **支持向量和间隔边界**
   $$
   H_1 : w \cdot x + b = 1 \\
   H_2 : w \cdot x + b = -1
   $$
   在$H_1$和$H_2$上的点就是支持向量。$H_1$和$H_2$之间形成一条长带，长带的宽度称为间隔。间隔依赖于分离超平面的法向量$w$，**等于$\frac{2}{||w||}$，**$H_1$和$H_2$称为间隔边界。在**决定分离超平面时只有支持向量起作用**，而其他实例点井不起作用。

### 7.1.4 学习的对偶算法

为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用**拉格朗日对偶性**。这样做的优点，一是对偶问题往往更容易求解；二是自然引入核函数，进而推广到非线性分类问题。

引入拉格朗日乘子并构建拉格朗日函数，见p120-121。原始问题的对偶问题是极大极小问题。

![定理7.2](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E7%90%867.2.PNG)

![算法7.2（线性可分支持向量机学习算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%957.2%EF%BC%88%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

在线性可分支持向量机中 ，由式 (7.25) 、式 (7.26) 可知，$w^*$和$b^*$只依赖于训练数据中对应于$\alpha_{i}^{*}>0$的样本点$(x_i,y_i)$，而其他样本点对$w^*$和$b^*$没有影响。我们**将训练数据中对应于$\alpha_{i}^{*}>0$的实例点$x_i \in \R^n$称为支持向量**。

![定义7.4（支持向量）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%897.4%EF%BC%88%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%EF%BC%89.PNG)



## 7.2 线性支持向量机与软间隔最大化

### 7.2.1 线性支持向量机

**线性不可分训练数据**

通常情况是，训练数据中有一些**特异点(outliers)**，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。可以为每个样本点$(x_i,y_i)$引进一个**松弛变量**$\xi_i >= 0$，使得函数间隔加上松弛变量大于等于1。**约束条件**变为
$$
y_i(w \cdot x_i + b) >= 1 - \xi_i
$$
同时对每个松弛变量支付一个代价，**目标函数**变为
$$
\frac{1}{2}||w||^2 + C \sum_{i=1}^{N}\xi_i
$$
$C>0$称为**惩罚参数**，最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2 $尽量小即间隔尽量大，同时使误分类点的个数尽量小。

![定义7.5（线性支持向量机）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%897.5%EF%BC%88%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%89.PNG)

### 7.2.2 学习的对偶算法

![定理7.3](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E7%90%867.3.PNG)

![算法7.3（线性支持向量机学习算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%957.3%EF%BC%88%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

### 7.2.3 支持向量

在线性不可分的情况下，将对偶问题的解中对应于$\alpha_i^* > 0$的样本点$(x_i, y_i)$的实例$x_i$称为支持向量(**软间隔的支持向量**)。

![图7.5 软间隔的支持向量](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE7.5%E8%BD%AF%E9%97%B4%E9%9A%94%E7%9A%84%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.PNG)

### 7.2.4 合页损失函数

**合页损失函数**
$$
L(y(w \cdot x + b))=[1 - y(w \cdot x + b)]_+ \\
[z]_+ = \left\{\begin{matrix}
 z,& z>0\\ 
 0,& z<=0
\end{matrix}\right.
$$
![定理7.4](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E7%90%867.4.PNG)

![图7.6 合页损失函数](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE7.6%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.PNG)



## 7.3 非线性支持向量机与核函数

### 7.3.1 核技巧

1. 非线性分类问题

   如果能用$R^n$中的一个超曲面将正负例正确分开，则称这个问题为**非线性可分问题**。

   用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。核技巧就属于这样的方法。

   核技巧应用到支持向量机，其基本想法就是通过一个**非线性变换将输入空间(欧氏空间$R^n$或离散集合)对应于一个特征空间(希尔伯特空间$H$)**，使得在输入空间中$R^n$的超曲面模型对应于特征空间$H$中的超平面模型(支持向量机)。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。

2. 核函数的定义

   ![定义7.6（核函数）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%897.6%EF%BC%88%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%89.PNG)

   核技巧的想法是，在学习和预测中**只定义核函数$K(x,z)$，而不显式地定义映射函数$\phi$**。通常，直接计算核函数$K(x,z)$比较容易，而通过$\phi(x)$和$\phi(z)$计算并不容易。

   特征空间$H$一般是高维，甚至是无穷维的。

   ![例7.3核函数和映射函数的关系](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E4%BE%8B7.3%E6%A0%B8%E5%87%BD%E6%95%B0%E5%92%8C%E6%98%A0%E5%B0%84%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB.PNG)

3. 核技巧在支持向量机中的应用

   使用核函数代替内积$x_i \cdot x_j$
   $$
   W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i,x_j) - \sum_{i=1}^{N}\alpha_i
   $$
   分类决策函数中的内积也可以替换
   $$
   f(x)=sign(\sum_{i=1}^{N} \alpha_i^*y_i\phi(x_i)\cdot\phi(x) + b^*) \\
   =sign(\sum_{i=1}^{N} \alpha_i^*y_i K(x_i,x) + b^*)
   $$
   学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧。

### 7.3.2 正定核

通常所说的核函数就是**正定核函数**(positive definite kernel function)。

详细见书p136~140

### 7.3.3 常用核函数

1. 多项式核函数(polynomial kernel function)
   $$
   K(x,z)=(x \cdot z + 1)
   $$
   对应的支持向量机是一个$p$次多项式分类器，分类决策函数
   $$
   f(x)=sign(\sum_{i=1}^{N} \alpha_i^*y_i (x_i \cdot x + 1)^p + b^*)
   $$

2. 高斯核函数(gaussian kernel function)
   $$
   K(x,z) = exp(- \frac{||x-z||^2}{2\sigma^2})
   $$
   对应的支持向量机是高斯径向基函数(radial basis function)分类器，分类决策函数
   $$
   f(x)=sign(\sum_{i=1}^{N} \alpha_i^*y_iexp(- \frac{||x-x_i||^2}{2\sigma^2}) + b^*)
   $$

3. 字符串核函数(string kernel function)

   核函数不仅可以定义在欧氏空间上，还可以定义在离散数据的集合上。

### 7.3.4 非线性支持向量分类机

![定义7.8（非线性支持向量机）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%897.8%EF%BC%88%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%89.PNG)

![算法7.4（非线性支持向量机学习算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%957.4%EF%BC%88%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%89.PNG)



## 7.4 序列最小最优化算法

**序列最小最优化(sequential minimal optimization, SMO)算法**。

SMO算法是一种启发式算法，其基本思路是:如果所有变量的解都满足此最优化问题的KKT条件(Karush-Kuhn-Tucker conditions)，那么这个最优化问题的解就得到了。因为 KKT 条件是该最优化问题的充分必要条件。否则，**选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题**。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度。**子问题有两个变量，一个是违反KKT条件最严重的那个，另一个由约束条件自动确定**。如此， SMO 算法将原问题不断分解为子问题井对子问题求解，进而达到求解原问题的目的。

注意：**子问题的两个变量中只有一个是自由变量**。假设$\alpha_1$和$\alpha_2$为两个变量，其余固定，有
$$
\alpha_1 = -y_1 \sum_{i=2}^{N}\alpha_i y_i
$$
如果$\alpha_2$确定，那么$\alpha_1$也随之确定。所以**子问题中同时更新两个变量**。

整个SMO算法包括两个部分：**求解两个变量二次规划的解析方法**和**选择变量的启发式方法**。

### 7.4.1 两个变量二次规划的求解方法

不失一般性，假设选择的两个变量是$\alpha_1,\alpha_2$，其他变量固定，于是SMO的最优化问题的子问题可以写成：
$$
\underset{\alpha_1,\alpha_2}{\min} W(\alpha_1, \alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1\alpha_2 - \\ (\alpha_1 + \alpha_2) + y_1\alpha_1\sum_{i=3}^{N}y_i\alpha_iK_{i1} + y_2\alpha_2\sum_{i=3}^{N}y_i\alpha_iK_{i2} \\
s.t. \alpha_1y_1 + \alpha_2y_2 = -\sum_{i=3}^{N}y_i\alpha_i = \varsigma \\
0 \leq \alpha_i \leq C, i=1,2
$$
推导见p144~p146

![定理7.6](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E7%90%867.6.PNG)

### 7.4.2 变量的选择方法

SMO算法在每个子问题中选择两个变量优化，其中**至少一个变量是违反KKT条件的**。

1. 第1个变量的选择

   SMO称选择第1个变量的过程为**外层循环**。外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量。

   KKT条件
   $$
   \alpha_i=0 \Leftrightarrow y_i g(x_i) \geq 1 \\
   0<\alpha_i<C \Leftrightarrow  y_i g(x_i) = 1 \\
   \alpha_i=C \Leftrightarrow y_i g(x_i) \leq 1 \\
   $$
   其中$g(x_i)=\sum_{j=1}^{N}\alpha_j y_j K(x_i, x_j) + b$

   该检验是在$\epsilon$范围内进行的。在检验过程中，外层循环**首先遍历所有满足条件$0<\alpha_i<C $的样本点**，即在间隔边界上的支持向量点，检验它们是否满足KKT条件。如果这些样本点都满足KKT条件，那么遍历整个训练集，检验它们是否满足KKT条件。

2. 第2个变量的选择

   SMO称选择第2个变量的过程为**内层循环**。一种简答的方法是**选择$\alpha_2$，使其对应的$|E_1-E_2|$最大**。特殊情况不能使得目标函数有足够的下降，则采取启发式规则继续选择$\alpha_2$。

   - 遍历间隔边界上的支持向量点，选择对应的$\alpha_2$，直到目标函数有明显的下降
   - 若找不到，则遍历训练数据集
   - 若找不到，则放弃第一个$\alpha_1$，回到外层循环重新选择$\alpha_1$

3. 计算阈值$b$和差值$E_i$

   见p148

### 7.4.3 SMO算法

![算法7.5（SMO算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%957.5%EF%BC%88SMO%E7%AE%97%E6%B3%95%EF%BC%89.PNG)