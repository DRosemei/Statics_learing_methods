# 第五章 决策树

决策树(decision tree)是一种基本的**分类与回归**方法。本章主要讨论用于分类的决策树。

其主要优点是模型具有可读性，分类速度快。

决策树学习通常包括3个步骤：**特征选择**、**决策树的生成**和**决策树的修剪**。这些决策树学习的思想主要来源于由 Quinlan在1986年提出的**ID3算法**和1993年提出的**C4.5算法**，以及由Breiman等人在1984 年提出的**CART**算法。



## 5.1 决策树模型与学习

### 5.1.1 决策树模型

**定义5.1（决策树）**分类决策树模型是一种描述对实例进行分类的树形结构。决策树由**结点**(node)和**有向边**(directed edge)组成。结点有两种类型：**内部结点**(internal node)和**叶结点**(leaf node) 。内部结点表示一个特征或属性，叶结点表示一个类。

### 5.1.2 决策树与if-then规则

决策树的路径或其对应if-then规则集合具有一个重要的性质：**互斥并且完备**。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。

### 5.1.3 决策树与条件概率分布

决策树还表示**给定特征条件下类的条件概率分布**。

![图5.2决策树对应于条件概率分布](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE5.2%E5%86%B3%E7%AD%96%E6%A0%91%E5%AF%B9%E5%BA%94%E4%BA%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83.PNG)

### 5.1.4 决策树学习

假设给定训练数据集
$$
D=\{(x_1,y_1),(x_2,y_3),...,(x_N,y_N)\}
$$
其中，$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$为输入实例（特征向量），$n$为特征个数，$y_i \in \{1,2,...,K\}$为类标记，$i=1,2,...,N$，$N$为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

决策树学习**本质上是从训练数据集中归纳出一组分类规则**。

**决策树的生成对应于模型的局部选择**，**决策树的剪枝对应于模型的全局选择**。



## 5.2 特征选择

### 5.2.1 特征选择问题

特征选择在于**选取对训练数据具有分类能力的特征**。这样可以提高决策树学习的效率

通常特征选择的准则是**信息增益**或**信息增益比**。

### 5.2.2 信息增益

**熵**，**条件熵**，**互信息**

**信息增益**(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。

**定义5.2（信息增益）** 特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即
$$
g(D,A)=H(D)-H(D|A)
$$
**信息增益等价于训练数据集中类与特征的互信息**。

给定训练数据集$D$和特征$A$。经验熵$H(D)$表示对数据集$A$进行分类的不确定性。而经验条件熵$H(D|A)$表示**在特征$A$给定的条件下对数据集$D$进行分类的不确定性**。那么它们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。

根据信息增益准则的特征选择方法是：对训练数据集(或子集)D，计算其每个特征的信息增益，井比较它们的大小，选择信息增益最大的特征。

![图5.2.2信息增益的一些定义](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE5.2.2%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9A%E4%B9%89.PNG)

![算法5.1（信息增益的算法](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.1%EF%BC%88%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%9A%84%E7%AE%97%E6%B3%95.PNG)

### 5.2.3 信息增益比

以信息增益作为划分训练数据集的特征，存在**偏向于选择取值较多的特征**的问题。使用信息增益比(information gain ratio)可以对这个问题进行校正。

![定义5.3（信息增益比](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%895.3%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%AF%94.PNG)



## 5.3 决策树的生成

### 5.3.1 ID3算法

![算法5.2 ID3的生成算法](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.2ID3%E7%9A%84%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95.PNG)

![算法5.3 C4.5的生成算法](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.3C4.5%E7%9A%84%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95.PNG)



## 5.4 决策树的剪枝

解决过拟合问题，考虑决策树的复杂度，对已生成的决策树进行剪枝。

决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)或代价函数(cost function)来实现。 设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,...,K$，$H_t(T)$为叶节点$t$上的经验熵，$\alpha>=0$为参数，则决策树学习的损失函数可以定义为
$$
C_\alpha(T)=\sum _{t=1} ^{|T|}N_tH_t(T)+\alpha|T|
$$
其中经验熵为
$$
H_t(T)=- \sum _{k} {\frac {N_{tk}}{N_t}}log{\frac {N_{tk}}{N_t}}
$$
损失函数中，将右端第一项记作
$$
C(T)=\sum _{t=1} ^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K} N_{tk} log{\frac {N_{tk}}{N_t}}
$$
这时有
$$
C_\alpha(T)=C(T)+\alpha|T|
$$
$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度。

![算法5.4（树的剪枝算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.4%EF%BC%88%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D%E7%AE%97%E6%B3%95%EF%BC%89.PNG)



## 5.5 CART算法

分类与回归树(classification and regression tree, CART)模型由Breiman等人1984年提出，是应用广泛的决策树学习方法。CART假设决策树是**二叉树**

CART算法由以下两步组成：
(1)决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
(2)决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪技的标准。

### 5.5.1 CART生成

决策树的生成就是**递归地构建二叉决策树**的过程。对**回归树用平方误差最小化准则**，对**分类树用基尼指数(Gini index)最小化准则**，进行特征选择，生成二叉树。

1. 回归树的生成

   假设$X$和$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训数据练集
   $$
   D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
   $$
   考虑如何生成回归树。

   一棵回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为$M$个单元$R_1,R_2,...,R_M$，并且在每个单元$R_M$上有一个固定的输出值$c_m$，渝是回归树模型可以表示为
   $$
   f(x)=\sum_{m=1}^{M}c_mI(x \in R_m)
   $$
   当输入空间的划分确定时，可以用平方误差$\sum_{x_i \in R_m}(y_i - f(x_i))^2$表示预测误差。

   易知单元$R_m$上的$c_m$的**最优值$\hat{c}_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值**，即
   $$
   \hat{c}_m=ave(y_i|x_i \in R_m)
   $$
   **问题是怎样对输入空间进行划分**。采用启发式的方法，选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点。
   $$
   R_1(j,s)=\{x|x^{(j)}<=s\}和R_2(j,s)=\{x|x^{(j)}>s\}
   $$
   然后寻找最优切分变量$j$和$s$。具体地，求解
   $$
   \underset{j,s}{min}[\underset{c_1}{min}\sum_{x_i \in R_1(j,s)}(y_i - c_1)^2 + \underset{c_2}{min}\sum_{x_i \in R_1(j,s)}(y_i - c_2)^2]
   $$
   对固定输入变量$j$可以找到最优切分点$s$。
   $$
   \hat{c}_1=ave(y_i|x_i \in R_1(j,s))和\hat{c}_2=ave(y_i|x_i \in R_2(j,s))
   $$
   **最小二乘回归树**

    ![算法5.5（最小二乘回归树生成算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.5%EF%BC%88%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92%E6%A0%91%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

   

2. 分类树的生成

   分类树用**基尼指数**选择最优特征，同时决定该特征的**最优二值切分点**。

   ![定义5.4（基尼指数）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%895.4%EF%BC%88%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0%EF%BC%89.PNG)

   基尼指数$Gini(D)$表示集合的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。

   ![图5.7二类分类中基尼指数、熵之半和分类误差率的关系](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%9B%BE5.7%E4%BA%8C%E7%B1%BB%E5%88%86%E7%B1%BB%E4%B8%AD%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0%E3%80%81%E7%86%B5%E4%B9%8B%E5%8D%8A%E5%92%8C%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE%E7%8E%87%E7%9A%84%E5%85%B3%E7%B3%BB.PNG)

   ![算法5.6（CART生成算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.6%EF%BC%88CART%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95%EF%BC%89.PNG)



### 5.5.2 CART剪枝

CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\{T_0,T_1,...,T_n\}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

![算法5.7（CART剪枝算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%955.7%EF%BC%88CART%E5%89%AA%E6%9E%9D%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

