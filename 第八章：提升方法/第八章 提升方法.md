# 第八章 提升方法

提升(boosting) 方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

## 8.3.1 提升方法AdaBoost算法

### 8.1.1 提升方法的基本思路

提升方法基于这样一种思想：对于一个复杂任务来说，将**多个专家的判断**进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠顶个诸葛亮"的道理。

**强可学习**和**弱可学习**。在概率近似正确(probably approximately correct, PAC)学习的框架中，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

### 8.1.2 AdaBoost算法

![算法8.1（AdaBoost）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%958.1%EF%BC%88AdaBoost%EF%BC%89.PNG)

## 8.2 AdaBoost算法的训练误差分析

![定理8.1（AdaBoost的训练误差界）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E7%90%868.1%EF%BC%88AdaBoost%E7%9A%84%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E7%95%8C%EF%BC%89.PNG)

这一定理说明，可以在每一轮选取适当的$G_m$使得$Z_m$小，从而使训练误差下降最快。对二类分类问题，有如下结果：

![定理8.2（二类分类问题AdaBoost的训练误差界）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E7%90%868.2%EF%BC%88%E4%BA%8C%E7%B1%BB%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98AdaBoost%E7%9A%84%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E7%95%8C%EF%BC%89.PNG)

![推论8.1](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E6%8E%A8%E8%AE%BA8.1.PNG)

表明在此条件下的AdaBoost的训练误差是**以指数速率下降**的。

AdaBoost算法不需要知道下界$\gamma$。

## 8.3 AdaBoost算法的解释

AdaBoost算法还有 另一个解释，即可以认为AdaBoost算法是**模型为加法模型**、**损失函数为指数函数**、**学习算法为前向分步算法**时的二类分类学习方法。

### 8.3.1 前向分步算法

加法模型
$$
f(x)=\sum_{m=1}^{M} \beta_m b(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。学习加法模型$f(x)$成为经验风险极小化即损失函数极小化的问题：
$$
\underset{\beta_m,\gamma_m}{min} \sum_{i=1}^{N}L(y_i, \sum_{m=1}^{M} \beta_m b(x_i;\gamma_m))
$$
**前向分步算法**解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，**每一步只学一个基函数及其系数**，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：
$$
\underset{\beta,\gamma}{min} \sum_{i=1}^{N} L(y_i, \beta b(x_i; \gamma))
$$
![算法8.2（前向分步算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%958.2%EF%BC%88%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

### 8.3.2 前向分布算法与AdaBoost

定理8.3 **AdaBoost算法是前向分步加法算法的特例**。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。

## 8.4 提升树

提升树是**以分类树或回归树为基本分类器**的提升方法。提升树被认为是统计学习中性能最好的方法之一

### 8.4.1 提升树模型

提升方法实际采用加法模型(即基函数的线性组合)与前向分步算法。以决策树为基函数的提升方法称为提升树(boosting tree) 对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。

提升树模型可以表示为决策树的加法模型：
$$
f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)
$$
其中，$T(x;\Theta_m)$表示决策树，$\Theta_m$为决策树的参数，$M$为树的个数。

### 8.4.2 提升树算法

提升树算法采用前向分步算法。首先确定初始提升树$f_0(x)=0$，第$m$步的模型是：
$$
f_m(x) = f_{m-1}(x) + T(x;\Theta_m)
$$
其中，$f_{m-1}$为当前模型，通过经验风险极小化确定下一棵决策树参数$\Theta_m$：
$$
\hat{\Theta}_m = \arg \underset{\Theta_m}{\min} \sum_{i=1}^{N}L(y_i, f_{m-1}(x_i) + T(x_i;\Theta_m))
$$
用**平方误差损失函数的回归问题**，用**指数损失函数的分类问题**以及用一般损失函数的一般决策问题。

对于**回归问题的提升树算法来说，只需简单的拟合当前模型的残差**。

![算法8.3（回归问题的提升树算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%958.3%EF%BC%88%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E7%9A%84%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

### 8.4.3 梯度提升

当损失函数是平方损失和指数损失函数时，每一步优化是很简单的 但对**一般损失函数**而言，往往每一步优化并不那么容易。针对这一问题，Freidman提出了**梯度提升**(gradient boosting)算法。这是**利用最速下降法的近似方法**，其关键是**利用损失函数的负梯度在当前模型的值**
$$
-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}
$$
作为**回归问题提升树算法中的残差的近似值**，拟合一个回归树。

![算法8.4（梯度提升算法）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%958.4%EF%BC%88%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95%EF%BC%89.PNG)

算法第一步初始化，估计使损失函数极小化的常数值。它是只有一个根结点的树。