# 第1章 统计学习及监督学习概论

## 1.1 统计学习

统计学习由监督学习 (supervised learning) 、无监督学习(unsupervised learning) 和强化学习 (reinforcement learning) 等组成。

统计学习方法的三要素，简称为模型(model)、策略(strategy) 和算法(algorithm)

## 1.2 统计学习分类

**监督学习**：输入空间、特征空间和输出空间

输入的第i个特征向量
$$
x_{i} = (x_{i}^{1}, x_{i}^{2},...,x_{i}^{n})^{T}
$$
联合概率分布：$P(X,Y)$

假设空间：模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间 (hypothesis space)

监督学习的模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数$Y=f(X)$来表示。对具体的输入进行相应的输出预测时，写作$P(y|x)$或$y=f(x)$。



**无监督学习**：无监督学习(unsupervised learning) 是指从无标注数据中学习预测模型的机器学习问题。无监督学习的本质是学习数据中的**统计规律或潜在结构**。模型可以实现对**数据的聚类、降维或概率估计**。



**强化学习**：（reinforcement learning) 是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程(Markov decision process) ，智能系统能观测到的是与环境互动得到的数据序列，强化学习的本质是学习最优的序贯决策。



**半监督与主动学习**：半监督学习（semi-supervised earning) 是指利用标注数据和未标注数据学习预测模型的机器学习问题。

主动学习(active learning) 是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据，往往是随机得到的，可以看作是"被动学习" 主动学习的目标是找出对学习最有帮助的实例让教师标注，以较小的标注代价，达到较好的学习效果



**按模型分类**：概率模型（$P(y|x)$）与非概率模型（$y=f(x)$），线性模型与非线性模型，参数化模型与非参数化模型

**按算法分类**：在线学习（online learning）和批量学习（batch learning）

**按技巧分类**：**贝叶斯学习**(Bayesian learning) ，又称为贝叶斯推理 (Bayesian inference) ，是统计学、机器学习中重要的方法。其主要想法是，在概率模型的学习和推理中，利用贝叶斯定理，**计算在给定数据条件下模型的条件概率，即后验概率**，并应用这个原理进行模型的估计，以及对数据的预测。将模型、未观测要素及其参数用变量表示，使用模型的先验分布是贝叶斯学习的特点。

假设随机变量D表示数据，随机变量θ表示模型参数。根据贝叶斯定理，可以用 以下公式计算后验概率 $P(\theta|D)$:
$$
P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(D)}
$$
预测时：
$$
P(x|D)=\int{P(x|\theta,D)P(\theta|D)}d\theta
$$
假设先验分布是均匀分布，取后验概率最大，就能从贝叶斯估计得到极大似然估计。

![贝叶斯估计与极大似然估计](https://github.com/DRosemei/statistical_learing_methods/blob/master/imgs/%E5%9B%BE1.6.PNG)

**频率学派和贝叶斯学派**：作者：Xiangyu Wang
链接：https://www.zhihu.com/question/20587681/answer/41436978
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

频率学派和贝叶斯学派最大的差别其实产生于对参数空间的认知上。所谓参数空间，就是你关心的那个参数可能的取值范围。频率学派（其实就是当年的Fisher）并不关心参数空间的所有细节，他们相信数据都是在这个空间里的”某个“参数值下产生的（虽然你不知道那个值是啥），所以他们的方法论一开始就是从**“哪个值最有可能是真实值”**这个角度出发的。于是就有了**最大似然（maximum likelihood）**以及**置信区间（confidence interval）**这样的东西，你从名字就可以看出来他们关心的就是我有多大把握去圈出那个唯一的真实参数。而贝叶斯学派恰恰相反，他们关心参数空间里的每一个值，因为他们觉得我们又没有上帝视角，怎么可能知道哪个值是真的呢？所以**参数空间里的每个值都有可能是真实模型使用的值**，区别只是概率不同而已。于是他们才会引入**先验分布（prior distribution）**和**后验分布（posterior distribution）**这样的概念来设法找出参数空间上的每个值的概率。最好诠释这种差别的例子就是想象如果你的后验分布是双峰的，频率学派的方法会去选这两个峰当中较高的那一个对应的值作为他们的最好猜测，而贝叶斯学派则会同时报告这两个值，并给出对应的概率

​				 	**核方法**：是使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习。有一些线性模型的学习方法基于相似度计算， 更具体地，向量内积计算。核方法可以把它们扩展到非线性模型的学习，使其应用范 围更广泛。



## 1.3 统计学习三要素

方法=模型+策略+算法

### **模型**：

假设空间可以定义为决策函数的集合：$F=\{f|Y=f(X)\}$，或者条件概率的集合：$F=\{P|P(Y|X)\}$

### **策略**：

**损失函数**和**风险函数**

损失函数值越小，模型就越好。由于模型的输入、输出(X,Y)是随机变量，遵循联合分布$P(X,Y)$，所以损失函数的期望是
$$
R_{exp}(f)=E_{P}[L(Y,f(X))]=\int_{x×y}L(y,f(x))P(x,y)dxdy
$$
这是**理论上模型 $f(X)$ 关于联合分布 $P(X,Y)$ 的平均意义下的损失**，称为**风险函数**(risk function) 或**期望损失** (expected loss)。一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就成为 个病态问题 (ill-formed problem)

给定一个训练数据集，**模型$f(X)$关于训练数据集的平均损失**称为**经验风险**(empirical risk)或**经验损失** (empirical loss)，记作$R_{emp}$:
$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_{i}, f(x_{i}))
$$
**经验风险最小化**和**结构风险最小化**：

在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式可以确定。经验风险最小化（empirical risk minimization, ERM) 的策略认为，经验风险最小的模型是最优的模型。

结构风险最小化(structural risk minimization, SRM) 是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化 (regularization) 。结构风险在经验风险上加上表示模型复杂度的正则化项(regularizer )或罚项(penalty term) 。



### 算法：

统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。



## 1.4 模型评估与模型选择

模型评估：训练误差和测试误差

过拟合

![训练误差和测试误差与模型复杂度的关系](https://github.com/DRosemei/statistical_learing_methods/blob/master/imgs/%E5%9B%BE1.9.PNG)



## 1.5 正则化和交叉验证

正则化：L1，L2范数

奥卡姆剃刀原理应用于模型选择时变为以下想法:在所有可能选择的模型中，能够很好地解释己知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，正则化项对于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率



交叉验证：简单交叉验证，S折交叉验证，留一交叉验证

## 1.6 泛化能力

泛化误差：
$$
R_{exp}(\hat{f})=E_{p}[L(Y,\hat{f}(X))]=\int_{x×y}L(y,\hat{f}(x))P(x,y)dxdy
$$
泛化误差上界：性质：:它是样本容量的函数，当样本容量增加时，泛化上界趋于0; 它是假设空间容量 （capacity) 函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。

**定理1.1**：

![泛化误差上界](https://github.com/DRosemei/statistical_learing_methods/blob/master/imgs/%E5%AE%9A%E7%90%861.1.PNG)



## 1.7 生成模型与判别模型

监督学习：

决策函数：$Y=f(X)$

条件概率分布：$P(Y|X)$

监督学习方法又可以分为**生成方法** (generative approach) 和**判别方法**（discriminative approach) 所学到的模型分别称为生成模型(generative model)和判别模型(discriminative model)



生成方法由数据学习联合概率分布$P(X,Y)$，求出条件概率分布$P(Y|X)$
$$
P(Y|X)=\frac{P(X,Y)}{P(X)}
$$
判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测模型，即判别模型。

## 1.8 监督学习应用

分类问题、标注问题和回归问题

标注问题的输入是个观测序列，输出是一个标记序列或状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。常用：隐马尔可夫模型、条件随机场

回归模型最常见的损失函数是平方损失函数，可以由最小二乘法求解。



