# 第四章 朴素贝叶斯法

朴素贝叶斯（naïve Bayes）法是基于**贝叶斯定理**与**特征条件独立假设**的**分类**方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出。

**注：朴素贝叶斯法和贝叶斯估计是不同的概念**。

## 4.1 朴素贝叶斯法的学习与分类

### 4.1.1 基本方法

![4.1 一些前提定义](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%AE%9A%E4%B9%894.1%E4%B8%80%E4%BA%9B%E5%89%8D%E6%8F%90%E5%AE%9A%E4%B9%89.PNG)

朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$，具体地学习以下先验概率以及条件概率分布。

**先验概率**：
$$
P(Y=c_k), k=1,2,...,K
$$
**条件概率分布**：
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k), k=1,2,...,K
$$
得到联合概率分布。

条件概率分布有指数级数量的参数，其估计实际是不可行的。

朴素贝叶斯法对条件概率分布做了**条件独立性假设**，由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k) \\
= \prod _{j=1} ^{n}P(X^{(j)}=x^{(j)}|Y=c_k)
$$
**条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的**。这一假设使朴素贝叶斯
法变得简单，但有时会**牺牲一定的分类准确率**。

朴素贝叶斯法分类，计算后验概率：
$$
P(Y=c_k|X=x)=\frac {P(X=x|Y=c_k)P(Y=c_k)} {\sum _k P(X=x|Y=c_k)P(Y=c_k)}
$$
朴素贝叶斯分类器可表示为：


$$
y=f(x)=argmax _{c_x}\frac {P(Y=c_k)\prod _{j=1} ^{n}P(X^{(j)}=x^{(j)}|Y=c_k)} {\sum _kP(Y=c_k)\prod _{j=1} ^{n}P(X^{(j)}=x^{(j)}|Y=c_k)}\\
=argmax _{c_x}P(Y=c_k)\prod _{j=1} ^{n}P(X^{(j)}=x^{(j)}|Y=c_k)
$$

### 4.1.2 后验概率最大化含义

![4.1.2后验概率最大化的含义](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E5%90%AB%E4%B9%894.1.2%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E6%9C%80%E5%A4%A7%E5%8C%96%E7%9A%84%E5%90%AB%E4%B9%89.PNG)



## 4.2 朴素贝叶斯法的参数估计

### 4.2.1 极大似然估计

**先验概率$P(Y=c_k)$的极大似然估计**是
$$
P(Y=c_k)=\frac {\sum _{i=1} ^{N} I(y_i-c_k)}{N}, k=1,2,...,K
$$
设第$j$个特征$x_i^{(j)}$可能取值的集合为$\{a_{j1},a_{j2},...,a_{jS_j}\}$，则条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：
$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)} \\
j=1,2,...,n;\\
l=1,2,...,S_j;\\
k=1,2,...,K
$$
式中，$x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值；$I$为指示函数；

### 4.2.2 学习与分类算法

![算法4.1（朴素贝叶斯算法（naive Bayes algorithm）](https://raw.githubusercontent.com/DRosemei/statistical_learing_methods/master/imgs/%E7%AE%97%E6%B3%954.1%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95.PNG)

### 4.2.3 贝叶斯估计

**使用极大似然估计可能出现要估计的概率值为0的情况**，会影响后验概率的计算。解决这一问题的方法是采用**贝叶斯估计**。**条件概率的贝叶斯估计**是
$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda}\\
\lambda >= 0
$$
等价于在随机变量各个取值的频数上赋予一个正数$\lambda > 0$。$\lambda=0$为极大似然估计。通常取$\lambda=1$，这时候称为**拉普拉斯平滑**。

**先验概率的贝叶斯估计**为：
$$
P(Y=c_k)=\frac {\sum _{i=1} ^{N} I(y_i=c_k)+\lambda}{N+K\lambda}
$$
**课后习题**

https://blog.csdn.net/xiaoxiao_wen/article/details/54097917